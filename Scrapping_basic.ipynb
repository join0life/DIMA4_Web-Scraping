{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee96e75d-80d1-4dc7-8186-f1391bd8c04c",
   "metadata": {},
   "source": [
    "# requests 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb02dd35-656d-498b-a699-55b9a48fe2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46e6414d-ea52-4d0c-9d8b-9c07d1c86647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "URL = 'https://www.naver.com'\n",
    "response = requests.get(URL) # get 방식으로 요청\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b012166-26ae-4d6a-9625-77fb0d8a108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a085f99-952e-4e8a-adca-7d65fa8acc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://search.naver.com/search.naver'\n",
    "query = {'query':'python'}\n",
    "response = requests.get(URL, params=query)\n",
    "print(response.status_code)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dc077d-2b47-4723-9a72-314bfd365cbd",
   "metadata": {},
   "source": [
    "# user-agent 값 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17880a-d6b3-4d07-ba0c-1ead1a276f5b",
   "metadata": {},
   "source": [
    "- 로봇이 아님을 나타내기 위해서 user-agent라는 값을 header에 넣어서 보냄\n",
    "- 직접적인 URL 주소로 요청 시 웹 사이트에서 웹 크롤링을 통해 접근한 것을 감지하고 접속을 차단하게 됨\n",
    "- user-agent 헤더값을 포함하여 요청하면 브라우저를 통해 요청하는 것으로 인식되어 해결\n",
    "- 웹 브라우저 실행 -> F12 개발자 모드 진입 -> Console에 navigator.userAgent 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2410f382-841e-4444-9894-6572a31fcc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장완료!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "URL ='http://www.google.com/search'\n",
    "params = {'q':'python'}\n",
    "headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0'}\n",
    "\n",
    "response = requests.get(URL, params=params)# headers=headers)\n",
    "response.raise_for_status() #응답 코드가 200이 아니면 오류내고 멈춤\n",
    "\n",
    "result = response.text\n",
    "with open('mygoogle.html','w', encoding ='utf-8') as f:\n",
    "    f.write(result)\n",
    "print('저장완료!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a0d83-861a-42c3-80c6-bb92fd042bca",
   "metadata": {},
   "source": [
    "## [실습] 네이버 데이터랩에서 실시간 인기 검색어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e989109-5990-458d-8acc-a4be9d86284e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트위드자켓\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('https://datalab.naver.com')\n",
    "html_text = response.text\n",
    "\n",
    "temp = html_text.split('<em class=\"num\">1</em>')[1]\n",
    "temp = temp.split('<span class=\"title\">')[1]\n",
    "temp = temp.split('</span>')[0]\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5971fe73-1e4d-4050-ae87-af11b8b96545",
   "metadata": {},
   "source": [
    "# BeautifulSoup 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0410a2-d9a0-44c1-9ae9-d74193e860ee",
   "metadata": {},
   "source": [
    "## Parser별 출력 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5774bedf-1970-44ff-9806-faf3de5b6619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\user\\anaconda3\\lib\\site-packages (4.9.3)\n"
     ]
    }
   ],
   "source": [
    "# !pip install html5lib\n",
    "# !pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b8c54b3-0fb5-43fc-82d8-b3f6d93f42a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html.parser\n",
      "<a></a>\n",
      "-------------------------------------------------\n",
      "lxml\n",
      "<html><body><a></a></body></html>\n",
      "-------------------------------------------------\n",
      "xml\n",
      "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
      "<a/>\n",
      "-------------------------------------------------\n",
      "html5lib\n",
      "<html><head></head><body><a><p></p></a></body></html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup('<a></p>','html.parser')\n",
    "print('html.parser')\n",
    "print(soup)\n",
    "print('-'*49)\n",
    "\n",
    "soup = BeautifulSoup('<a></p>','lxml')\n",
    "print('lxml')\n",
    "print(soup)\n",
    "print('-'*49)\n",
    "\n",
    "soup = BeautifulSoup('<a></p>','xml')\n",
    "print('xml')\n",
    "print(soup)\n",
    "print('-'*49)\n",
    "\n",
    "soup = BeautifulSoup('<a></p>','html5lib')\n",
    "print('html5lib')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352a9849-d630-4528-8d78-9315c493ab64",
   "metadata": {},
   "source": [
    "## 기본 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bdd9f94-d043-4c6b-90e2-41114b0d4f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>웹 크롤러 - 위키백과, 우리 모두의 백과사전</title>\n",
      " 이 문서는 2023년 4월 30일 (일) 18:34에 마지막으로 편집되었습니다.\n",
      "<a class=\"mw-jump-link\" href=\"#bodyContent\">본문으로 이동</a>\n",
      "#bodyContent\n",
      "<a href=\"/wiki/%EA%B5%AC%EA%B8%80%EB%B4%87\" title=\"구글봇\">구글봇</a>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = 'https://ko.wikipedia.org/wiki/%EC%9B%B9_%ED%81%AC%EB%A1%A4%EB%9F%AC'\n",
    "response = requests.get(URL)\n",
    "\n",
    "# soup 객체 생성\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# 태그를 이용한 접근\n",
    "print(soup.title)\n",
    "print(soup.footer.ul.li.text)\n",
    "\n",
    "# 태그와 속성을 이용한 접근\n",
    "print(soup.a) # soup 객체에서 첫 번째로 만나는 a 태그 출력\n",
    "# print(soup.a['id']) # 만약 속성이 존재하지 않으면 에러 발생\n",
    "print(soup.a['href'])\n",
    "\n",
    "# find() 함수를 이용한 태그 내의 다양한 속성을 이용한 접근\n",
    "print(soup.find('a', attrs={'title':'구글봇'})) # a 태그 중 title 속성의 값이 '구글봇'인 데이터 검색"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f225cc88-b88f-452a-b441-625447959c34",
   "metadata": {},
   "source": [
    "## 자식 노드들을 반복 가능한 객체로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e10bef-db01-4b8a-ad57-4cc91a1d342c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<p align=\"center\" class=\"a\">text1</p>\n",
      "\n",
      "\n",
      "<p align=\"center\" class=\"b\">text2</p>\n",
      "\n",
      "\n",
      "<p align=\"center\" class=\"c\">text3</p>\n",
      "\n",
      "\n",
      "<div>\n",
      "<img height=\"200\" src=\"/source\" width=\"300\"/>\n",
      "</div>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Web Scrapping</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"a\" align=\"center\">text1</p>\n",
    "        <p class=\"b\" align=\"center\">text2</p>\n",
    "        <p class=\"c\" align=\"center\">text3</p>\n",
    "        <div>\n",
    "            <img src=\"/source\" width = \"300\" height = \"200\">\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "contents = soup.find('body')\n",
    "# print(contents)\n",
    "for child in contents.children:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f303e32a-f309-4f79-8b61-f5a7dd753a1c",
   "metadata": {},
   "source": [
    "## 자신을 포함한 부모 노드까지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c11cc2e3-c4b8-4948-9c55-ab8ffb492fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<img height=\"200\" src=\"/source\" width=\"300\"/>\n",
      "\n",
      "<div>\n",
      "<img height=\"200\" src=\"/source\" width=\"300\"/>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Web Scrapping</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"a\" align=\"center\">text1</p>\n",
    "        <p class=\"b\" align=\"center\">text2</p>\n",
    "        <p class=\"c\" align=\"center\">text3</p>\n",
    "        <div>\n",
    "            <img src=\"/source\" width = \"300\" height = \"200\">\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "contents = soup.find('body')\n",
    "img_tag = contents.find('img')\n",
    "print(img_tag)\n",
    "print()\n",
    "print(img_tag.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a76fbfe-6e55-4356-9ffa-7292092f1bdc",
   "metadata": {},
   "source": [
    "## 특정 부모 노드까지 검색해서 올라감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8800cab-ceda-4060-91ea-2306cb32d3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<body>\n",
      "<p align=\"center\" class=\"a\">text1</p>\n",
      "<p align=\"center\" class=\"b\">text2</p>\n",
      "<p align=\"center\" class=\"c\">text3</p>\n",
      "<div>\n",
      "<img height=\"200\" src=\"/source\" width=\"300\"/>\n",
      "</div>\n",
      "</body>\n"
     ]
    }
   ],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Web Scrapping</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"a\" align=\"center\">text1</p>\n",
    "        <p class=\"b\" align=\"center\">text2</p>\n",
    "        <p class=\"c\" align=\"center\">text3</p>\n",
    "        <div>\n",
    "            <img src=\"/source\" width = \"300\" height = \"200\">\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "contents = soup.find('body')\n",
    "img_tag = contents.find('img')\n",
    "print(img_tag.find_parent('body'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871ef10d-ea32-4a0b-96d5-c27da02572e3",
   "metadata": {},
   "source": [
    "## 형제 노드 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f962dbda-d76d-48d6-af9d-0124a726a6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p align=\"center\" class=\"b\">text2</p>\n",
      "--바로 다음 형제 노드--\n",
      "<p align=\"center\" class=\"c\">text3</p>\n",
      "--모든 다음 형제 노드--\n",
      "[<p align=\"center\" class=\"c\">text3</p>, <div>\n",
      "<img height=\"200\" src=\"/source\" width=\"300\"/>\n",
      "</div>]\n",
      "--바로 이전 형제 노드--\n",
      "<p align=\"center\" class=\"a\">text1</p>\n",
      "--모든 이전 형제 노드--\n",
      "[<p align=\"center\" class=\"a\">text1</p>]\n"
     ]
    }
   ],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Web Scrapping</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"a\" align=\"center\">text1</p>\n",
    "        <p class=\"b\" align=\"center\">text2</p>\n",
    "        <p class=\"c\" align=\"center\">text3</p>\n",
    "        <div>\n",
    "            <img src=\"/source\" width = \"300\" height = \"200\">\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "p_tag = soup.find('p', attrs={'class':'b'})\n",
    "print(p_tag)\n",
    "print('--바로 다음 형제 노드--')\n",
    "print(p_tag.find_next_sibling())\n",
    "print('--모든 다음 형제 노드--')\n",
    "print(p_tag.find_next_siblings())\n",
    "print('--바로 이전 형제 노드--')\n",
    "print(p_tag.find_previous_sibling())\n",
    "print('--모든 이전 형제 노드--')\n",
    "print(p_tag.find_previous_siblings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d71a41-0243-4c4f-a8f7-d2d557b1d9df",
   "metadata": {},
   "source": [
    "## 검색: find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814253f6-5c6d-47bc-8d82-48a333624da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get('https://naver.com')\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "print(soup.find('title'))\n",
    "print(soup.find('a'))\n",
    "print(soup.find(id='search')) # id 속성의 값이 'search'인 정보를 가져옴, soup.find(attrs={'id':'search'}) 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ec8d91-ae73-4ed1-8869-2be048e0af43",
   "metadata": {},
   "source": [
    "## 검색: find_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d7682e1-8dba-4ff6-98f7-c8049f7973b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[<a href=\"#topAsideButton\"><span>상단영역 바로가기</span></a>, <a href=\"#shortcutArea\"><span>서비스 메뉴 바로가기</span></a>]\n"
     ]
    }
   ],
   "source": [
    "a_tags = soup.find_all('a', limit = 2)\n",
    "print(len(a_tags))\n",
    "print(a_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410877ad-65ba-4e7e-a4e8-b552b5d3c5de",
   "metadata": {},
   "source": [
    "[실습] 네이버 뉴스 페이지에서 언론사 목록 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da47a602-473b-4609-88d3-602e84370e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['월간산', 'MBC', '헬스조선', '한겨레', '뉴스1', '기자협회보', '블로터', 'SBS Biz', '여성신문', '헤럴드경제']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://news.naver.com')\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "result = soup.find_all('h4', attrs={'class':'channel'})\n",
    "# print(len(result))\n",
    "# print(result[0])\n",
    "# print(list(result[0].children))\n",
    "press_list = [list(tag.children)[0] for tag in result]\n",
    "print(press_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8a4c359-8aa6-44a0-83c3-fbf8eef306b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['신동아', '주간조선', '월간산', '한경비즈니스', '시사IN', '매경이코노미', '주간동아', '주간경향', '시사저널', '이코노미스트']\n"
     ]
    }
   ],
   "source": [
    "res = requests.get('https://news.naver.com')\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "result = soup.find_all('div', attrs={'class':'cjs_age_name'})\n",
    "press_list = [tag.text for tag in result] #tag.text 또는 tag.get_text()\n",
    "print(press_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d478ef-85b6-4f8e-a7f1-2e765cb39df6",
   "metadata": {},
   "source": [
    "## 검색: select_one(), select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f440b1c-47c8-41cf-afd7-48f75a367e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"show\" href=\"/page/KITA_MAIN\">KITA 무역아카데미</a>\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('http://www.tradecampus.com')\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "print(soup.select_one('div > a'))\n",
    "result = soup.select('div > a')\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd3245de-da3c-4362-bce9-c4b77dec9889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h3 class=\"tit\">공지사항</h3>\n"
     ]
    }
   ],
   "source": [
    "print(soup.select_one('body > div > div.wrapper.main_page > div.renew_main > div.col-12 > div > div.renew_main_notice > div > div > h3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8f19b3f-e95c-4d35-b2f8-5a25d902efad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024년 무역아카데미 주요강좌 안내\n"
     ]
    }
   ],
   "source": [
    "# tradecampus.com 메인 페이지 공지사항 2번째 항목 선택\n",
    "# :nth-child(#)을 지우면 해당 요소(element)의 모든 요소를 가져온다\n",
    "notice = soup.select('body > div > div.wrapper.main_page > div.renew_main > div.col-12 > div > div.renew_main_notice > div > ul > li:nth-child(2) > p > a')\n",
    "print(notice[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f090b9-2592-46b9-92b9-120ef098bee2",
   "metadata": {},
   "source": [
    "## 텍스트 가져오기: text, get_text()\n",
    "- 검색 결과에서 태그를 제외한 텍스트만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f713dc3a-750d-4986-82d8-1158207ab75d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "고객센터\n",
      "\n",
      "\n",
      "오프라인 교육, e러닝\n",
      "02-6000-5378/5379\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "운영시간\n",
      "평일 09:00~18:00 (주말/공휴일 : 휴무)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 고객센터 영역 텍스트 가져오기\n",
    "tag = soup.find('div', attrs = {'class':'serviceInfo'})\n",
    "# print(tag)\n",
    "print(tag.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "04324d03-edc4-449d-996e-b77681024964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/weven_template_repository/theme/KITAAC/1/resource/img/ico_sns_facebook.png\n",
      "/weven_template_repository/theme/KITAAC/1/resource/img/ico_sns_facebook.png\n"
     ]
    }
   ],
   "source": [
    "# 풋터에 있는 kita 로고 이미지 추출\n",
    "tag = soup.find('img', attrs={'class':'mobile_icon black'})\n",
    "print(tag['src'])\n",
    "print(tag.get('src'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27aacce-739b-4de9-9632-316a9af3f163",
   "metadata": {},
   "source": [
    "## 텍스트 가져오기: string\n",
    "- 검색 결과에서 **태그 안에 또 다른 태그가 없는 경우** 해당 내용을 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca976d86-c7ba-4144-99f9-de547922b829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오프라인 교육, e러닝\n"
     ]
    }
   ],
   "source": [
    "tag = soup.find('div', attrs={'class':'serviceInfo'})\n",
    "tag = tag.find('span')\n",
    "print(tag.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75202f79-2e0a-4c2f-99d2-f0827569beb7",
   "metadata": {},
   "source": [
    "[실습] 네이버 웹툰 제목 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38dc738-c7e3-47f7-ab18-3e4cf860db7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52361b6-5845-49b1-8653-310578b37ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a56351da-ae29-4cef-8b0f-750e14ab709b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = 'https://comic.naver.com/webtoon'\n",
    "res = requests.get(URL)\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "# webtoon_titles = soup.find_all('span', attrs ={'class':'ContentTitle__title--e3qXt'})\n",
    "# print(len(webtoon_titles))\n",
    "# webtoon_titles = soup.select_one('#container > div.component_wrap.type2 > div.WeekdayMainView__daily_all_wrap--UvRFc > div.WeekdayMainView__daily_all_item--DnTAH.WeekdayMainView__is_active--NSACG > ul > li:nth-child(1) > div > a > span > span')\n",
    "# print(webtoon_titles) # 이렇게 하면 안됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22a6e00-fc22-4a73-8cb4-8cb949224516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from  webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "\n",
    "URL = 'https://comic.naver.com/webtoon'\n",
    "driver.get(URL)\n",
    "time.sleep(4) # 동적으로 생성되는 페이지의 내용이 완성될 때까지 대기\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "# 요일별 전체 웹툰 CSS 선택자\n",
    "temp = soup.select_one('#container > div.component_wrap.type2 > div.WeekdayMainView__daily_all_wrap--UvRFc')\n",
    "# 요일별 div 태그 검색\n",
    "temp = temp.find_all('ul', attrs={'class':'WeekdayMainView__daily_list--R52q0'})\n",
    "week = ['월','화','수','목','금','토','일']\n",
    "for i,w in enumerate(temp):\n",
    "    print(f'===== {week[i]}요 웹툰 =====')\n",
    "    webtoon_list = w.find_all('li', attrs = {'class':'DailyListItem__item--LP6_T'}) #li는 각 웹툰 하나하나\n",
    "    for webtoon in webtoon_list:\n",
    "        print(webtoon.find('span', attrs={'class':'text'}).text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73111a1b-1c18-4367-85b3-e1e01daba69f",
   "metadata": {},
   "source": [
    "[실습] 메가박스 영화정보 사이트에서 영화 포스터 다운로드 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14d9b549-9694-49f9-bc2d-a521b9390091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "# 사전 테스트: 박스 오피스 1위 영화 포스터 이미지 가져오기\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import ChromiumOptions\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "URL = 'https://www.megabox.co.kr/movie'\n",
    "driver.get(URL)\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "poster_img = soup.select('#movieList > li > div.movie-list-info > img')\n",
    "print(len(poster_img))\n",
    "poster_img_src = poster_img[0].get('src')\n",
    "\n",
    "import requests\n",
    "res = requests.get(poster_img_src)\n",
    "with open('poster.jpg', 'wb') as f:\n",
    "    f.write(res.content)\n",
    "print('End')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8014fdf5-ae31-4ccc-8deb-15d48fdf651c",
   "metadata": {},
   "source": [
    "[문제] 메가박스 영화 사이트에서 첫 페이지에 있는 모든 영화 포스트 이미지 수집하기\n",
    "- 메가박스 영화 사이트 첫 페이지에 있는 20개의 영화 포스트 이미지 수집\n",
    "- 현재 작업 디렉토리 밑에 'poster_img' 폴더가 없는 경우 폴더를 생성한다. (os 패키지 적용)\n",
    "- 저장되는 각 포스터 이미지의 파일 이름은 영화 제목으로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3f7db65-43d2-43b6-87a3-383b053802d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "폴더가 존재함\n",
      "1 : https://img.megabox.co.kr/SharedImg/2024/03/08/cXQEF1oKHFLu6EvP16zSZGjHUJoYSAT2_420.jpg\n",
      "2 : https://img.megabox.co.kr/SharedImg/2024/03/15/rfWfBieOgyEguDNCjgvzkiT6fqJRwu7l_420.jpg\n",
      "3 : https://img.megabox.co.kr/SharedImg/2024/03/28/daNUgWGbgmQ7hIRsIlJ85DXUHpWhyj40_420.jpg\n",
      "4 : https://img.megabox.co.kr/SharedImg/2024/03/19/TEKBnolSgyXtoyDRLXMPQToR717LUcfL_420.jpg\n",
      "5 : https://img.megabox.co.kr/SharedImg/2024/02/22/s7Ica1Ow0MEP0U7l57tOjO1DXexU2N9E_420.jpg\n",
      "6 : https://img.megabox.co.kr/SharedImg/2024/03/22/ntJBTMX63n5aD3HogAOMD6DtBfGiLT5a_420.jpg\n",
      "7 : https://img.megabox.co.kr/SharedImg/2024/03/29/4LUZ72p3FhEUqR3OgDEk841Ut55WmJJj_420.jpg\n",
      "8 : https://img.megabox.co.kr/SharedImg/2024/03/15/hmfQLwxxlkFILBcgZQjXZYRqI6KofQGk_420.jpg\n",
      "9 : https://img.megabox.co.kr/SharedImg/2024/03/11/7QVpllXiIeNhPFD1fa8UV2bQ6o7lv71a_420.jpg\n",
      "10 : https://img.megabox.co.kr/SharedImg/2024/03/19/n4B15lKM6DohSjd2ub3bByvQfNeqwdGD_420.jpg\n",
      "11 : https://img.megabox.co.kr/SharedImg/2024/03/20/tUnvSS6SSta7pqQYrcufMOHw00pviUtX_420.jpg\n",
      "12 : https://img.megabox.co.kr/SharedImg/2024/02/07/LVp8bCwlBWpI9BHacDmo6Vun9nm5n9PW_420.jpg\n",
      "13 : https://img.megabox.co.kr/SharedImg/2024/03/19/vrfAJUoemwwlw7kXFuFlYubUHhP4Pgos_420.jpg\n",
      "14 : https://img.megabox.co.kr/SharedImg/2024/03/18/bCcK5IeYEagMg75txVlxcqCjdChN456n_420.jpg\n",
      "15 : https://img.megabox.co.kr/SharedImg/2024/03/21/bMexyZw0N8N8nBcrPtjC7Jt0KLYNhaHO_420.jpg\n",
      "16 : https://img.megabox.co.kr/SharedImg/2024/02/26/kFf35GeLMd3Ni2fhigw9JZ8MMiu5tvF3_420.jpg\n",
      "17 : https://img.megabox.co.kr/SharedImg/2024/03/14/qFNWK95TrptuCUrtq4B938hmhfSflzkD_420.jpg\n",
      "18 : https://img.megabox.co.kr/SharedImg/2024/03/15/VMNPl1TlwiWohpiMGjt0batQaqAklnSw_420.jpg\n",
      "19 : https://img.megabox.co.kr/SharedImg/2024/02/08/yWsIaw2YRxrqrKcUOoHWHn4nzklTmOPv_420.jpg\n",
      "20 : https://img.megabox.co.kr/SharedImg/2024/03/22/aZhNH447xp0kcfz6DvfQb0BIMsgGVYEd_420.jpg\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver import ChromeOptions, Chrome\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "URL = 'https://www.megabox.co.kr/movie'\n",
    "driver.get(URL)\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "# 포스터 이미지를 가지고 있는 모든 img 태그를 검색\n",
    "poster_imgs = soup.find_all('img', attrs = {'class':'poster lozad'})\n",
    "# print(len(poster_imgs))\n",
    "\n",
    "# 이미지를 저장할 폴더 생성 \n",
    "import os\n",
    "img_dir = './poster_img/'\n",
    "if not os.path.exists(img_dir):\n",
    "    os.mkdir(img_dir)\n",
    "    print('폴더 생성 완료')\n",
    "else:\n",
    "    print('폴더가 존재함')\n",
    "\n",
    "for i,poster in enumerate(poster_imgs, 1):\n",
    "    title = poster.get('alt')\n",
    "    img_url = poster.get('src')\n",
    "\n",
    "    print(i, ':', img_url)\n",
    "    img_res = requests.get(img_url)\n",
    "\n",
    "    if ':' in title:\n",
    "        title = title.replace(':',' ')\n",
    "\n",
    "    with open(img_dir+f'[{i}].{title}.jpg', 'wb') as f:\n",
    "        f.write(img_res.content) # 바이너리 데이터의 콘텐츠 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf57ba46-2fc8-4cab-b07c-f4e5305ff74f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
