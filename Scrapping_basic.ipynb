{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee96e75d-80d1-4dc7-8186-f1391bd8c04c",
   "metadata": {},
   "source": [
    "# requests 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb02dd35-656d-498b-a699-55b9a48fe2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46e6414d-ea52-4d0c-9d8b-9c07d1c86647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "URL = 'https://www.naver.com'\n",
    "response = requests.get(URL) # get 방식으로 요청\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b012166-26ae-4d6a-9625-77fb0d8a108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a085f99-952e-4e8a-adca-7d65fa8acc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://search.naver.com/search.naver'\n",
    "query = {'query':'python'}\n",
    "response = requests.get(URL, params=query)\n",
    "print(response.status_code)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dc077d-2b47-4723-9a72-314bfd365cbd",
   "metadata": {},
   "source": [
    "# user-agent 값 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17880a-d6b3-4d07-ba0c-1ead1a276f5b",
   "metadata": {},
   "source": [
    "- 로봇이 아님을 나타내기 위해서 user-agent라는 값을 header에 넣어서 보냄\n",
    "- 직접적인 URL 주소로 요청 시 웹 사이트에서 웹 크롤링을 통해 접근한 것을 감지하고 접속을 차단하게 됨\n",
    "- user-agent 헤더값을 포함하여 요청하면 브라우저를 통해 요청하는 것으로 인식되어 해결\n",
    "- 웹 브라우저 실행 -> F12 개발자 모드 진입 -> Console에 navigator.userAgent 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2410f382-841e-4444-9894-6572a31fcc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장완료!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "URL ='http://www.google.com/search'\n",
    "params = {'q':'python'}\n",
    "headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0'}\n",
    "\n",
    "response = requests.get(URL, params=params)# headers=headers)\n",
    "response.raise_for_status() #응답 코드가 200이 아니면 오류내고 멈춤\n",
    "\n",
    "result = response.text\n",
    "with open('mygoogle.html','w', encoding ='utf-8') as f:\n",
    "    f.write(result)\n",
    "print('저장완료!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a0d83-861a-42c3-80c6-bb92fd042bca",
   "metadata": {},
   "source": [
    "## [실습] 네이버 데이터랩에서 실시간 인기 검색어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e989109-5990-458d-8acc-a4be9d86284e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트위드자켓\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('https://datalab.naver.com')\n",
    "html_text = response.text\n",
    "\n",
    "temp = html_text.split('<em class=\"num\">1</em>')[1]\n",
    "temp = temp.split('<span class=\"title\">')[1]\n",
    "temp = temp.split('</span>')[0]\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5971fe73-1e4d-4050-ae87-af11b8b96545",
   "metadata": {},
   "source": [
    "# BeautifulSoup 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0410a2-d9a0-44c1-9ae9-d74193e860ee",
   "metadata": {},
   "source": [
    "## Parser별 출력 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5774bedf-1970-44ff-9806-faf3de5b6619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\user\\anaconda3\\lib\\site-packages (4.9.3)\n"
     ]
    }
   ],
   "source": [
    "# !pip install html5lib\n",
    "# !pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b8c54b3-0fb5-43fc-82d8-b3f6d93f42a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html.parser\n",
      "<a></a>\n",
      "-------------------------------------------------\n",
      "lxml\n",
      "<html><body><a></a></body></html>\n",
      "-------------------------------------------------\n",
      "xml\n",
      "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
      "<a/>\n",
      "-------------------------------------------------\n",
      "html5lib\n",
      "<html><head></head><body><a><p></p></a></body></html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup('<a></p>','html.parser')\n",
    "print('html.parser')\n",
    "print(soup)\n",
    "print('-'*49)\n",
    "\n",
    "soup = BeautifulSoup('<a></p>','lxml')\n",
    "print('lxml')\n",
    "print(soup)\n",
    "print('-'*49)\n",
    "\n",
    "soup = BeautifulSoup('<a></p>','xml')\n",
    "print('xml')\n",
    "print(soup)\n",
    "print('-'*49)\n",
    "\n",
    "soup = BeautifulSoup('<a></p>','html5lib')\n",
    "print('html5lib')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352a9849-d630-4528-8d78-9315c493ab64",
   "metadata": {},
   "source": [
    "## 기본 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bdd9f94-d043-4c6b-90e2-41114b0d4f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>웹 크롤러 - 위키백과, 우리 모두의 백과사전</title>\n",
      " 이 문서는 2023년 4월 30일 (일) 18:34에 마지막으로 편집되었습니다.\n",
      "<a class=\"mw-jump-link\" href=\"#bodyContent\">본문으로 이동</a>\n",
      "#bodyContent\n",
      "<a href=\"/wiki/%EA%B5%AC%EA%B8%80%EB%B4%87\" title=\"구글봇\">구글봇</a>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = 'https://ko.wikipedia.org/wiki/%EC%9B%B9_%ED%81%AC%EB%A1%A4%EB%9F%AC'\n",
    "response = requests.get(URL)\n",
    "\n",
    "# soup 객체 생성\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# 태그를 이용한 접근\n",
    "print(soup.title)\n",
    "print(soup.footer.ul.li.text)\n",
    "\n",
    "# 태그와 속성을 이용한 접근\n",
    "print(soup.a) # soup 객체에서 첫 번째로 만나는 a 태그 출력\n",
    "# print(soup.a['id']) # 만약 속성이 존재하지 않으면 에러 발생\n",
    "print(soup.a['href'])\n",
    "\n",
    "# find() 함수를 이용한 태그 내의 다양한 속성을 이용한 접근\n",
    "print(soup.find('a', attrs={'title':'구글봇'})) # a 태그 중 title 속성의 값이 '구글봇'인 데이터 검색"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f225cc88-b88f-452a-b441-625447959c34",
   "metadata": {},
   "source": [
    "## 자식 노드들을 반복 가능한 객체로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e10bef-db01-4b8a-ad57-4cc91a1d342c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<p align=\"center\" class=\"a\">text1</p>\n",
      "\n",
      "\n",
      "<p align=\"center\" class=\"b\">text2</p>\n",
      "\n",
      "\n",
      "<p align=\"center\" class=\"c\">text3</p>\n",
      "\n",
      "\n",
      "<div>\n",
      "<img height=\"200\" src=\"/source\" width=\"300\"/>\n",
      "</div>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Web Scrapping</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"a\" align=\"center\">text1</p>\n",
    "        <p class=\"b\" align=\"center\">text2</p>\n",
    "        <p class=\"c\" align=\"center\">text3</p>\n",
    "        <div>\n",
    "            <img src=\"/source\" width = \"300\" height = \"200\">\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "contents = soup.find('body')\n",
    "# print(contents)\n",
    "for child in contents.children:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f303e32a-f309-4f79-8b61-f5a7dd753a1c",
   "metadata": {},
   "source": [
    "## 자신을 포함한 부모 노드까지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c11cc2e3-c4b8-4948-9c55-ab8ffb492fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<img height=\"200\" src=\"/source\" width=\"300\"/>\n",
      "\n",
      "<div>\n",
      "<img height=\"200\" src=\"/source\" width=\"300\"/>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Web Scrapping</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"a\" align=\"center\">text1</p>\n",
    "        <p class=\"b\" align=\"center\">text2</p>\n",
    "        <p class=\"c\" align=\"center\">text3</p>\n",
    "        <div>\n",
    "            <img src=\"/source\" width = \"300\" height = \"200\">\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "contents = soup.find('body')\n",
    "img_tag = contents.find('img')\n",
    "print(img_tag)\n",
    "print()\n",
    "print(img_tag.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a76fbfe-6e55-4356-9ffa-7292092f1bdc",
   "metadata": {},
   "source": [
    "## 특정 부모 노드까지 검색해서 올라감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8800cab-ceda-4060-91ea-2306cb32d3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<body>\n",
      "<p align=\"center\" class=\"a\">text1</p>\n",
      "<p align=\"center\" class=\"b\">text2</p>\n",
      "<p align=\"center\" class=\"c\">text3</p>\n",
      "<div>\n",
      "<img height=\"200\" src=\"/source\" width=\"300\"/>\n",
      "</div>\n",
      "</body>\n"
     ]
    }
   ],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Web Scrapping</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"a\" align=\"center\">text1</p>\n",
    "        <p class=\"b\" align=\"center\">text2</p>\n",
    "        <p class=\"c\" align=\"center\">text3</p>\n",
    "        <div>\n",
    "            <img src=\"/source\" width = \"300\" height = \"200\">\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "contents = soup.find('body')\n",
    "img_tag = contents.find('img')\n",
    "print(img_tag.find_parent('body'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871ef10d-ea32-4a0b-96d5-c27da02572e3",
   "metadata": {},
   "source": [
    "## 형제 노드 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f962dbda-d76d-48d6-af9d-0124a726a6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p align=\"center\" class=\"b\">text2</p>\n",
      "--바로 다음 형제 노드--\n",
      "<p align=\"center\" class=\"c\">text3</p>\n",
      "--모든 다음 형제 노드--\n",
      "[<p align=\"center\" class=\"c\">text3</p>, <div>\n",
      "<img height=\"200\" src=\"/source\" width=\"300\"/>\n",
      "</div>]\n",
      "--바로 이전 형제 노드--\n",
      "<p align=\"center\" class=\"a\">text1</p>\n",
      "--모든 이전 형제 노드--\n",
      "[<p align=\"center\" class=\"a\">text1</p>]\n"
     ]
    }
   ],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Web Scrapping</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"a\" align=\"center\">text1</p>\n",
    "        <p class=\"b\" align=\"center\">text2</p>\n",
    "        <p class=\"c\" align=\"center\">text3</p>\n",
    "        <div>\n",
    "            <img src=\"/source\" width = \"300\" height = \"200\">\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "p_tag = soup.find('p', attrs={'class':'b'})\n",
    "print(p_tag)\n",
    "print('--바로 다음 형제 노드--')\n",
    "print(p_tag.find_next_sibling())\n",
    "print('--모든 다음 형제 노드--')\n",
    "print(p_tag.find_next_siblings())\n",
    "print('--바로 이전 형제 노드--')\n",
    "print(p_tag.find_previous_sibling())\n",
    "print('--모든 이전 형제 노드--')\n",
    "print(p_tag.find_previous_siblings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d71a41-0243-4c4f-a8f7-d2d557b1d9df",
   "metadata": {},
   "source": [
    "## 검색: find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814253f6-5c6d-47bc-8d82-48a333624da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get('https://naver.com')\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "print(soup.find('title'))\n",
    "print(soup.find('a'))\n",
    "print(soup.find(id='search')) # id 속성의 값이 'search'인 정보를 가져옴, soup.find(attrs={'id':'search'}) 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ec8d91-ae73-4ed1-8869-2be048e0af43",
   "metadata": {},
   "source": [
    "## 검색: find_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d7682e1-8dba-4ff6-98f7-c8049f7973b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[<a href=\"#topAsideButton\"><span>상단영역 바로가기</span></a>, <a href=\"#shortcutArea\"><span>서비스 메뉴 바로가기</span></a>]\n"
     ]
    }
   ],
   "source": [
    "a_tags = soup.find_all('a', limit = 2)\n",
    "print(len(a_tags))\n",
    "print(a_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410877ad-65ba-4e7e-a4e8-b552b5d3c5de",
   "metadata": {},
   "source": [
    "[실습] 네이버 뉴스 페이지에서 언론사 목록 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da47a602-473b-4609-88d3-602e84370e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['디지털데일리', '한경비즈니스', '농민신문', '전자신문', 'MBC', '시사IN', '블로터', 'YTN', '헬스조선', '아시아경제']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://news.naver.com')\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "result = soup.find_all('h4', attrs={'class':'channel'})\n",
    "# print(len(result))\n",
    "# print(result[0])\n",
    "# print(list(result[0].children))\n",
    "\n",
    "press_list = [list(tag.children)[0] for tag in result]\n",
    "print(press_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a582517f-d4d6-48de-b027-db623896bef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "채널A\n"
     ]
    }
   ],
   "source": [
    "print(list(result[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8a4c359-8aa6-44a0-83c3-fbf8eef306b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['신동아', '주간조선', '월간산', '한경비즈니스', '시사IN', '매경이코노미', '주간동아', '주간경향', '시사저널', '이코노미스트']\n"
     ]
    }
   ],
   "source": [
    "res = requests.get('https://news.naver.com')\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "result = soup.find_all('div', attrs={'class':'cjs_age_name'})\n",
    "press_list = [tag.text for tag in result] #tag.text 또는 tag.get_text()\n",
    "print(press_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d478ef-85b6-4f8e-a7f1-2e765cb39df6",
   "metadata": {},
   "source": [
    "## 검색: select_one(), select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f440b1c-47c8-41cf-afd7-48f75a367e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"show\" href=\"/page/KITA_MAIN\">KITA 무역아카데미</a>\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('http://www.tradecampus.com')\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "print(soup.select_one('div > a'))\n",
    "result = soup.select('div > a')\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd3245de-da3c-4362-bce9-c4b77dec9889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h3 class=\"tit\">공지사항</h3>\n"
     ]
    }
   ],
   "source": [
    "print(soup.select_one('body > div > div.wrapper.main_page > div.renew_main > div.col-12 > div > div.renew_main_notice > div > div > h3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8f19b3f-e95c-4d35-b2f8-5a25d902efad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제31기 수출입기업 영업이익의 보전을 위한 원스톱 환리스크관리 개강(3/21)\n"
     ]
    }
   ],
   "source": [
    "# tradecampus.com 메인 페이지 공지사항 2번째 항목 선택\n",
    "# :nth-child(#)을 지우면 해당 요소(element)의 모든 요소를 가져온다\n",
    "notice = soup.select('body > div > div.wrapper.main_page > div.renew_main > div.col-12 > div > div.renew_main_notice > div > ul > li:nth-child(2) > p > a')\n",
    "print(notice[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f090b9-2592-46b9-92b9-120ef098bee2",
   "metadata": {},
   "source": [
    "## 텍스트 가져오기: text, get_text()\n",
    "- 검색 결과에서 태그를 제외한 텍스트만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f713dc3a-750d-4986-82d8-1158207ab75d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "고객센터\n",
      "\n",
      "\n",
      "오프라인 교육, e러닝\n",
      "02-6000-5378/5379\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "운영시간\n",
      "평일 09:00~18:00 (주말/공휴일 : 휴무)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 고객센터 영역 텍스트 가져오기\n",
    "tag = soup.find('div', attrs = {'class':'serviceInfo'})\n",
    "# print(tag)\n",
    "print(tag.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "04324d03-edc4-449d-996e-b77681024964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/weven_template_repository/theme/KITAAC/1/resource/img/ico_sns_facebook.png\n",
      "/weven_template_repository/theme/KITAAC/1/resource/img/ico_sns_facebook.png\n"
     ]
    }
   ],
   "source": [
    "# 풋터에 있는 kita 로고 이미지 추출\n",
    "tag = soup.find('img', attrs={'class':'mobile_icon black'})\n",
    "print(tag['src'])\n",
    "print(tag.get('src'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27aacce-739b-4de9-9632-316a9af3f163",
   "metadata": {},
   "source": [
    "## 텍스트 가져오기: string\n",
    "- 검색 결과에서 **태그 안에 또 다른 태그가 없는 경우** 해당 내용을 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca976d86-c7ba-4144-99f9-de547922b829",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오프라인 교육, e러닝\n"
     ]
    }
   ],
   "source": [
    "tag = soup.find('div', attrs={'class':'serviceInfo'})\n",
    "tag = tag.find('span')\n",
    "print(tag.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75202f79-2e0a-4c2f-99d2-f0827569beb7",
   "metadata": {},
   "source": [
    "[실습] 네이버 웹툰 제목 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38dc738-c7e3-47f7-ab18-3e4cf860db7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52361b6-5845-49b1-8653-310578b37ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a56351da-ae29-4cef-8b0f-750e14ab709b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = 'https://comic.naver.com/webtoon'\n",
    "res = requests.get(URL)\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "# webtoon_titles = soup.find_all('span', attrs ={'class':'ContentTitle__title--e3qXt'})\n",
    "# print(len(webtoon_titles))\n",
    "# webtoon_titles = soup.select_one('#container > div.component_wrap.type2 > div.WeekdayMainView__daily_all_wrap--UvRFc > div.WeekdayMainView__daily_all_item--DnTAH.WeekdayMainView__is_active--NSACG > ul > li:nth-child(1) > div > a > span > span')\n",
    "# print(webtoon_titles) # 이렇게 하면 안됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22a6e00-fc22-4a73-8cb4-8cb949224516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from  webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "\n",
    "URL = 'https://comic.naver.com/webtoon'\n",
    "driver.get(URL)\n",
    "time.sleep(4) # 동적으로 생성되는 페이지의 내용이 완성될 때까지 대기\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "# 요일별 전체 웹툰 CSS 선택자\n",
    "temp = soup.select_one('#container > div.component_wrap.type2 > div.WeekdayMainView__daily_all_wrap--UvRFc')\n",
    "#container > div.component_wrap.type2 > div.WeekdayMainView__daily_all_wrap--UvRFc\n",
    "\n",
    "# 요일별 div 태그 검색\n",
    "temp = temp.find_all('ul', attrs={'class':'WeekdayMainView__daily_list--R52q0'})\n",
    "week = ['월','화','수','목','금','토','일']\n",
    "for i,w in enumerate(temp):\n",
    "    print(f'===== {week[i]}요 웹툰 =====')\n",
    "    webtoon_list = w.find_all('li', attrs = {'class':'DailyListItem__item--LP6_T'}) #li는 각 웹툰 하나하나\n",
    "    for webtoon in webtoon_list:\n",
    "        print(webtoon.find('span', attrs={'class':'text'}).text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73111a1b-1c18-4367-85b3-e1e01daba69f",
   "metadata": {},
   "source": [
    "[실습] 메가박스 영화정보 사이트에서 영화 포스터 다운로드 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14d9b549-9694-49f9-bc2d-a521b9390091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "# 사전 테스트: 박스 오피스 1위 영화 포스터 이미지 가져오기\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import ChromiumOptions\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "URL = 'https://www.megabox.co.kr/movie'\n",
    "driver.get(URL)\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "poster_img = soup.select('#movieList > li > div.movie-list-info > img')\n",
    "print(len(poster_img))\n",
    "poster_img_src = poster_img[0].get('src')\n",
    "\n",
    "import requests\n",
    "res = requests.get(poster_img_src)\n",
    "with open('poster.jpg', 'wb') as f:\n",
    "    f.write(res.content)\n",
    "print('End')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8014fdf5-ae31-4ccc-8deb-15d48fdf651c",
   "metadata": {},
   "source": [
    "[문제] 메가박스 영화 사이트에서 첫 페이지에 있는 모든 영화 포스트 이미지 수집하기\n",
    "- 메가박스 영화 사이트 첫 페이지에 있는 20개의 영화 포스트 이미지 수집\n",
    "- 현재 작업 디렉토리 밑에 'poster_img' 폴더가 없는 경우 폴더를 생성한다. (os 패키지 적용)\n",
    "- 저장되는 각 포스터 이미지의 파일 이름은 영화 제목으로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3f7db65-43d2-43b6-87a3-383b053802d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "폴더가 존재함\n",
      "1 : https://img.megabox.co.kr/SharedImg/2024/03/08/cXQEF1oKHFLu6EvP16zSZGjHUJoYSAT2_420.jpg\n",
      "2 : https://img.megabox.co.kr/SharedImg/2024/03/15/rfWfBieOgyEguDNCjgvzkiT6fqJRwu7l_420.jpg\n",
      "3 : https://img.megabox.co.kr/SharedImg/2024/03/28/daNUgWGbgmQ7hIRsIlJ85DXUHpWhyj40_420.jpg\n",
      "4 : https://img.megabox.co.kr/SharedImg/2024/03/19/TEKBnolSgyXtoyDRLXMPQToR717LUcfL_420.jpg\n",
      "5 : https://img.megabox.co.kr/SharedImg/2024/02/22/s7Ica1Ow0MEP0U7l57tOjO1DXexU2N9E_420.jpg\n",
      "6 : https://img.megabox.co.kr/SharedImg/2024/03/22/ntJBTMX63n5aD3HogAOMD6DtBfGiLT5a_420.jpg\n",
      "7 : https://img.megabox.co.kr/SharedImg/2024/03/29/4LUZ72p3FhEUqR3OgDEk841Ut55WmJJj_420.jpg\n",
      "8 : https://img.megabox.co.kr/SharedImg/2024/03/15/hmfQLwxxlkFILBcgZQjXZYRqI6KofQGk_420.jpg\n",
      "9 : https://img.megabox.co.kr/SharedImg/2024/03/11/7QVpllXiIeNhPFD1fa8UV2bQ6o7lv71a_420.jpg\n",
      "10 : https://img.megabox.co.kr/SharedImg/2024/03/19/n4B15lKM6DohSjd2ub3bByvQfNeqwdGD_420.jpg\n",
      "11 : https://img.megabox.co.kr/SharedImg/2024/03/20/tUnvSS6SSta7pqQYrcufMOHw00pviUtX_420.jpg\n",
      "12 : https://img.megabox.co.kr/SharedImg/2024/02/07/LVp8bCwlBWpI9BHacDmo6Vun9nm5n9PW_420.jpg\n",
      "13 : https://img.megabox.co.kr/SharedImg/2024/03/19/vrfAJUoemwwlw7kXFuFlYubUHhP4Pgos_420.jpg\n",
      "14 : https://img.megabox.co.kr/SharedImg/2024/03/18/bCcK5IeYEagMg75txVlxcqCjdChN456n_420.jpg\n",
      "15 : https://img.megabox.co.kr/SharedImg/2024/03/21/bMexyZw0N8N8nBcrPtjC7Jt0KLYNhaHO_420.jpg\n",
      "16 : https://img.megabox.co.kr/SharedImg/2024/02/26/kFf35GeLMd3Ni2fhigw9JZ8MMiu5tvF3_420.jpg\n",
      "17 : https://img.megabox.co.kr/SharedImg/2024/03/14/qFNWK95TrptuCUrtq4B938hmhfSflzkD_420.jpg\n",
      "18 : https://img.megabox.co.kr/SharedImg/2024/03/15/VMNPl1TlwiWohpiMGjt0batQaqAklnSw_420.jpg\n",
      "19 : https://img.megabox.co.kr/SharedImg/2024/02/08/yWsIaw2YRxrqrKcUOoHWHn4nzklTmOPv_420.jpg\n",
      "20 : https://img.megabox.co.kr/SharedImg/2024/03/22/aZhNH447xp0kcfz6DvfQb0BIMsgGVYEd_420.jpg\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver import ChromeOptions, Chrome\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "URL = 'https://www.megabox.co.kr/movie'\n",
    "driver.get(URL)\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "# 포스터 이미지를 가지고 있는 모든 img 태그를 검색\n",
    "poster_imgs = soup.find_all('img', attrs = {'class':'poster lozad'})\n",
    "# print(len(poster_imgs))\n",
    "\n",
    "# 이미지를 저장할 폴더 생성 \n",
    "import os\n",
    "img_dir = './poster_img/'\n",
    "if not os.path.exists(img_dir):\n",
    "    os.mkdir(img_dir)\n",
    "    print('폴더 생성 완료')\n",
    "else:\n",
    "    print('폴더가 존재함')\n",
    "\n",
    "for i,poster in enumerate(poster_imgs, 1):\n",
    "    title = poster.get('alt') # poster['alt']\n",
    "    img_url = poster.get('src') # poster['src']\n",
    "\n",
    "    print(i, ':', img_url)\n",
    "    img_res = requests.get(img_url)\n",
    "\n",
    "    if ':' in title:\n",
    "        title = title.replace(':',' ')\n",
    "\n",
    "    with open(img_dir+f'[{i}].{title}.jpg', 'wb') as f:\n",
    "        f.write(img_res.content) # 바이너리 데이터의 콘텐츠 값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df277e8a-9184-4c11-94ce-f1d55aeceb79",
   "metadata": {},
   "source": [
    "[실습] 네이버 뉴스 사이트에서 경제 관련 언론사별 랭킹뉴스 추출하기\n",
    "- 언론사 이름에 '경제' 단어가 포함된 언론사의 랭킹 뉴스만 추출하여 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "01ed9dab-9b30-4790-8a9e-c78e226620f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "등록 언론사 개수:  82\n",
      "언론사: 한국경제\n",
      "1:'침착맨' 이말년, 53억 빌딩 사들이더니 결국…위기의 MCN\n",
      "2:\"23만원까지 나왔다\" 환호…신고가 행진에 '20만닉스' 성큼\n",
      "3:\"90분간 뺨 맞아\" 학폭 의혹 '여배우' 송하윤…\"일면식도 없다\" [전문]\n",
      "4:\"5조 또 쏜다\" 반도체에 미친 日…한국선 꿈도 못 꿀 일 [김일규의 재팬워치]\n",
      "5:남자랑 통화했다고 여동생 살해…지켜본 아빠, 촬영한 오빠\n",
      "\n",
      "언론사: 아시아경제\n",
      "1:방송 중 느닷없이 재생된 영상…당황한 아나운서 \"실수로 재생됐다\" 사과\n",
      "2:치료·수술할 때만 입국해 건보 혜택 '쏙'…내일부터 못한다\n",
      "3:혀도 발음도 꼬였던 앵커, 진짜 낮술했었네…방송국, 징계예정\n",
      "4:\"쓰레기\" \"후진 놈\"…거칠어지는 한동훈의 입, 왜?\n",
      "5:\"차량 등록 안해준다\"며 아파트 정문 막은 입주민 논란\n",
      "\n",
      "언론사: 매일경제\n",
      "1:“나가, 내집이야”…남편 말에 싸우고 나온 황정음, 이태원서 한 일\n",
      "2:300만원 넘는 침대 척척 팔더니…사상 처음 1등 올라선 ‘이 회사’\n",
      "3:강 건너면 강남인데 밤에 돌아다니기 무섭다는 이 동네…양꼬치 거리·대학생·새 아파트 표심 제각각 [민심로드2024, 광진을]\n",
      "4:“90분간 따귀 맞아” 송하윤, 학폭 의혹에 “일면식도 없다”\n",
      "5:‘연두색 번호판’에 벌벌 떨었나…럭셔리카 올해초 판매량 급감\n",
      "\n",
      "언론사: 한국경제TV\n",
      "1:\"삼성전자, 엔비디아 HBM 납품 논란 불필요\"\n",
      "2:자고나면 신고가...\"수익난 투자자 비중 89%\"\n",
      "3:'여행 못 가겠네'…1박당 추가 세금 붙는다\n",
      "4:'더 2024 K9' 출시…\"고급화에 중점\"\n",
      "5:잘 나가고 있었는데…바이오 개미들 '비명'\n",
      "\n",
      "언론사: 헤럴드경제\n",
      "1:\"90분간 뺨 때리고, 강제전학\" 학폭 의혹 여배우 누구?\n",
      "2:“애플워치보다 예쁜가?” 이게 겨우 8만원…삼성 역대급 시계 등장\n",
      "3:4년간 ‘이것’ 즐겨 마셨더니…‘체중 감량’에 효과 있더라!\n",
      "4:[영상]아파트서 자다 강풍에 밖으로 날아가…3명 중국 일가족 숨져\n",
      "5:9층서 물린 삼전개미 탈출, 이번엔 ‘가능 vs 불가능’…파죽지세로 10만전자 현실되나? [투자360]\n",
      "\n",
      "언론사: 서울경제\n",
      "1:인기 이 정도일 줄이야…인스타 개설 첫날 팔로워 28만명 넘는 '이 곳'\n",
      "2:낮술 한잔 하고 뉴스 진행? 혀 꼬인 앵커 '음주 생방송' 사실이었다\n",
      "3:\"엄마 놀이터에 친구가 없어\"…놀이 기구 대신 자리 잡는 어르신 의자들\n",
      "4:속고 또 속았었지만…이번엔 진짜 '10만 전자' 꿈 현실로?\n",
      "5:[속보]한동훈 \"정경심 억대 영치금… 조국도 감옥 가면 그럴것\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://news.naver.com/main/ranking/popularDay.naver')\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "news_list = soup.find_all('div', attrs = {'class':'rankingnews_box'})\n",
    "print('등록 언론사 개수: ', len(news_list))\n",
    "\n",
    "for news in news_list:\n",
    "    press_title = news.find('strong').text # strong 태그 하나밖에 없음\n",
    "    if '경제' in press_title:\n",
    "        print('언론사:', press_title)\n",
    "        press_news = news.find_all('div', attrs = {'class':'list_content'})\n",
    "        for i, ranking_news in enumerate(press_news, 1):\n",
    "            # print(f'{i}:{ranking_news.get_text()}')\n",
    "            print(f'{i}:{ranking_news.find(\"a\").get_text()}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d0dc45-69b0-4b78-b45c-21383e8066a8",
   "metadata": {},
   "source": [
    "# Selenium 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77792534-aab5-4284-8c63-c1e2396ae97f",
   "metadata": {},
   "source": [
    "## find_element() 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fe2fce6f-7660-48a9-9e8b-f2764c104e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<selenium.webdriver.remote.webelement.WebElement (session=\"26fa9700db1a98bcb632cad772e791b8\", element=\"f.4C9E78F35E2C89169A1DADA529B46DB1.d.2B18D557C92D61E4C696310E9A2A9B08.e.61\")>\n",
      "카페\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# 웹 드라이버 동적 다운로드 방식\n",
    "driver = Chrome(service = Service(ChromeDriverManager().install()), options = ChromeOptions())\n",
    "\n",
    "# [참고] 로컬 컴퓨터에 설치되어 있는 웹 드라이버 실행 방식\n",
    "# s = Service('d:\\DEV\\chromedriver\\chromedriver.exe')\n",
    "# driver = Chrome(service=s)\n",
    "\n",
    "driver.get('https://www.daum.net')\n",
    "ele = driver.find_element(by=By.LINK_TEXT, value='카페')\n",
    "print(ele)\n",
    "print(ele.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90549ffb-1cd8-407f-b3e2-61fbad045394",
   "metadata": {},
   "source": [
    "## 이벤트 제어하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7dc264-cfd3-4ecf-8805-380c946435f3",
   "metadata": {},
   "source": [
    "### click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69b3cc61-87fa-4ba1-bce6-fb0bb9ac2f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "\n",
    "driver.get('https://www.naver.com')\n",
    "import time\n",
    "time.sleep(4)\n",
    "ele = driver.find_element(by=By.CSS_SELECTOR, value='#shortcutArea > ul > li:nth-child(5) > a')\n",
    "ele.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b8ebbe-5e60-4fcd-b4bf-4391911c9377",
   "metadata": {},
   "source": [
    "### send_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9ad6e6c-a6b6-4198-b9c2-f8a120b6c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys # 특수키 사용을 위한 클래스\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "\n",
    "driver.get('https://www.naver.com')\n",
    "ele = driver.find_element(by=By.ID, value='query')\n",
    "ele.send_keys('python')\n",
    "ele.send_keys(Keys.ENTER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5227977b-e4d3-4744-a10f-2c249a44ff1d",
   "metadata": {},
   "source": [
    "[실습] 네이버 로그인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc1ebd1a-0ee8-4f94-8a43-85ffacc9914f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3183199756.py, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[14], line 30\u001b[1;36m\u001b[0m\n\u001b[1;33m    driver.execute_script(f'document.getElementById('keep').value=\"on\"')\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys # 특수키 사용을 위한 클래스\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "\n",
    "driver.get('https://www.naver.com')\n",
    "# ele = driver.find_element(by=By.CSS_SELECTOR, value='#account > div > a')\n",
    "# ele.click()\n",
    "ele = driver.find_element(by=By.CLASS_NAME, value='MyView-module__link_login___HpHMW')\n",
    "ele.click()\n",
    "import time\n",
    "time.sleep(4)\n",
    "my_id = 'cupshop'\n",
    "my_pw = 'QLCsksmsduwk10'\n",
    "\n",
    "# 로봇에 의해 클릭되지 못하도록 막았기 때문에 스크립트로 처리해야 함\n",
    "# ele = driver.find_element(by=By.ID, value='id')\n",
    "# ele.send_keys(my_id)\n",
    "\n",
    "# ele = driver.find_element(by=By.ID, value='pw')\n",
    "# ele.send_keys(my_pw)\n",
    "\n",
    "driver.execute_script(f\"document.getElementById('id').value='{my_id}'\") # DOM 객체가 document에 들어감. getElementById -> id 값으로 element 찾음\n",
    "driver.execute_script(f\"document.getElementById('pw').value='{my_pw}'\")\n",
    "time.sleep(3)\n",
    "#로그인 상태 유지 체크박스 클릭\n",
    "driver.execute_script(f\"document.getElementById('keep').value='on'\")\n",
    "time.sleep(3)\n",
    "ele = driver.find_element(by=By.CLASS_NAME, value='btn_login')\n",
    "ele.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e8497b-43f9-49e5-9725-d0cb01b07b15",
   "metadata": {},
   "source": [
    "## 웹 브라우저 자동 스크롤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14922eb4-6c67-4252-9e94-404e37eff260",
   "metadata": {},
   "source": [
    "### 구글에서 이미지 검색 후 검색 결과 6번 스크롤 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "296411dd-4f17-4b8a-a859-e2cbeb6726c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last height:  7126\n",
      "new_height:  13915\n",
      "------------------------------\n",
      "new_height:  20611\n",
      "------------------------------\n",
      "new_height:  27059\n",
      "------------------------------\n",
      "new_height:  30401\n",
      "------------------------------\n",
      "new_height:  30401\n",
      "------------------------------\n",
      "new_height:  30401\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "# 페이지가 로드될 때까지 기다리는 시간\n",
    "SCROLL_PAUSE_TIME = 2\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "driver.get('https://www.google.com')\n",
    "ele = driver.find_element(by=By.CLASS_NAME, value='gLFyf')\n",
    "ele.send_keys('python')\n",
    "ele.submit() #입력 폼을 서버로 전송할 때 \n",
    "\n",
    "driver.find_element(By.LINK_TEXT, '이미지').click()\n",
    "\n",
    "# 페이지가 로드될 때까지 기다림\n",
    "time.sleep(SCROLL_PAUSE_TIME)\n",
    "# 최초 스크롤 바의 높이값 읽기\n",
    "last_height = driver.execute_script('return document.body.scrollHeight')\n",
    "print('last height: ', last_height)\n",
    "for i in range(6):\n",
    "    # 윈도우의 스크롤 바를 0에서부터 가장 밑(scrollHeight)까지 이동\n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "    # 스크롤 바 이동으로 새로운 검색 결과가 로딩 후 변경된 새로운 스크롤 바의 높이값 읽기\n",
    "    new_height = driver.execute_script('return document.body.scrollHeight')\n",
    "    print('new_height: ', new_height)\n",
    "    print('-'*30)\n",
    "# 더 이상 스크롤 될 페이지가 없을 경우 scrollHeight 값의 변화가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b215d8b9-c82c-404a-ae2a-59f7ecf7a827",
   "metadata": {},
   "source": [
    "### 구글에서 이미지 검색 후 검색 결과 무한 스크롤하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff19241a-c7cd-43bf-bfb3-0953f8485d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last height : 6971, new_height : 13915, scroll count : 1\n",
      "last height : 13915, new_height : 20611, scroll count : 2\n",
      "last height : 20611, new_height : 27059, scroll count : 3\n",
      "last height : 27059, new_height : 30153, scroll count : 4\n",
      "last height : 30153, new_height : 30153, scroll count : 5\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "# 페이지가 로드될 때까지 기다리는 시간\n",
    "SCROLL_PAUSE_TIME = 2\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "driver.get('https://www.google.com')\n",
    "ele = driver.find_element(by=By.CLASS_NAME, value='gLFyf')\n",
    "ele.send_keys('python')\n",
    "ele.submit() #입력 폼을 서버로 전송할 때 \n",
    "\n",
    "driver.find_element(By.LINK_TEXT, '이미지').click()\n",
    "\n",
    "# 페이지가 로드될 때까지 기다림\n",
    "time.sleep(SCROLL_PAUSE_TIME)\n",
    "# 최초 스크롤 바의 높이값 읽기\n",
    "last_height = driver.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "#스크롤 횟수\n",
    "scroll_cnt = 0\n",
    "\n",
    "while True:\n",
    "    # 윈도우의 스크롤 바를 0에서부터 가장 밑(scrollHeight)까지 이동\n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
    "    scroll_cnt += 1\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "    # 스크롤 바 이동으로 새로운 검색 결과가 로딩 후 변경된 새로운 스크롤 바의 높이값 읽기\n",
    "    new_height = driver.execute_script('return document.body.scrollHeight')\n",
    "    print(f'last height : {last_height}, new_height : {new_height}, scroll count : {scroll_cnt}')\n",
    "    if last_height != new_height: # 계속해서 new_height 값이 변경되면\n",
    "        last_height = new_height\n",
    "    else: # 더 이상 new_height 값의 변동이 없다면\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1870f7c-daf1-410a-a6d4-c48e8b273a9e",
   "metadata": {},
   "source": [
    "### 구글에서 이미지 검색 후 썸네일 이미지 클릭하고 원본 이미지 주소 수집하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "528965d4-880d-4d67-9539-0e4a377f9efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last height : 7126, new_height : 13915, scroll count : 1\n",
      "last height : 13915, new_height : 20611, scroll count : 2\n",
      "last height : 20611, new_height : 26966, scroll count : 3\n",
      "last height : 26966, new_height : 30401, scroll count : 4\n",
      "last height : 30401, new_height : 30401, scroll count : 5\n",
      "449\n",
      "[1].https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTyQlAY-m5UXpwyX4z78Vpi4WFnNiU7e311PwoZU3kyXQ&s\n",
      "[2].https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTyQlAY-m5UXpwyX4z78Vpi4WFnNiU7e311PwoZU3kyXQ&s\n",
      "[3].https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTAu8ccQozYR93JU28Z_TQ0shBqt5JoyBB4FAgCGk3R&s\n",
      "[4].https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSMApZtXyv635ASXMhrib42OHQ3p27UsYlCTYDgWkSeOA&s\n",
      "[5].https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRFGXUdp0yroYt0J4ofF12BifaTS9epNkff2LnYW5MZ&s\n",
      "[6].https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTPYzrvt2jYgJdaImwVVwi5mr6vlB0vY7sqe__MSkX7Bg&s\n",
      "[7].https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT2DP-JJG1obIchPRU6acWjA117G09_QMxIy5f08reyYw&s\n",
      "[8].https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRa2aAhcBO5F-CL4c7A73HzvUK-7rxO-xQcSEbruRqdUw&s\n",
      "[9].https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRHh_ZcssZ8hhjl1VcwJ8tRQu46RkV7pmAnKOh5mNeK5Q&s\n",
      "[10].https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT42wjH_cRpDDS2kJPSGi7KrGae9mQA6oSgaa7_5PNQuw&s\n",
      "[11].https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTwWwRuQrMAU5rsWjfxfl3RbBpdQR7b9gCmGmHNQKj2Cg&s\n",
      "[12].https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQuEqxeKnScMZV-U7rH3vkpflcHcsxId5dS3G9Ay1i0Eg&s\n",
      "[13].https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR63s5HEH-p4Hwv-M43nBzLc9K1JhRs30DHtn4Z8FNO&s\n",
      "[14].https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT9lNtB6THrlrOcC9oU0DAQ47isjAwLpnDs3ZH6BJnq7Q&s\n",
      "[15].https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS0PfDCVkKnm_NUHM4Rl-pEczpldplteHC11mIbKlD3tw&s\n",
      "[16].https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS3KuO72Jhtf_BCxsX0ZEweREm7gDWdYfVU2oKuVctENA&s\n",
      "[17].https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ6Qlf0D77-lfMNYMNyFLawSIwEKN-RWUyTs1lcuQDUjQ&s\n",
      "[18].https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSC30ieBWsnagq5P-Q4E-5yv7B79PAmLeZ9FEqvZSCoyQ&s\n",
      "[19].https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTOPW7cYryaBolVXci10ftu-pkBVCXTCSoMAHxWLYUgbw&s\n",
      "[20].https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS0dNNa4mPGhCfp3-C4mrSp-m3LIYCnqZqxMW4arcUcBA&s\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "SCROLL_PAUSE_TIME = 2# 페이지가 로드될 때까지 기다리는 시간\n",
    "IMAGE_EXTRACT_NUM = 20 # 이미지 추출 개수\n",
    "SEARCH_KEYWORD = 'python' \n",
    "\n",
    "######## 웹 드라이버 실행 및 구글 접속 #######\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "driver.get('https://www.google.com')\n",
    "\n",
    "######## 검색 #######\n",
    "ele = driver.find_element(by=By.CLASS_NAME, value='gLFyf')\n",
    "ele.send_keys(SEARCH_KEYWORD)\n",
    "ele.submit() #입력 폼을 서버로 전송할 때 \n",
    "\n",
    "######## 이미지 검색결과 페이지 이동 #######\n",
    "driver.find_element(By.LINK_TEXT, '이미지').click()\n",
    "\n",
    "# 페이지가 로드될 때까지 기다림\n",
    "time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "######## 이미지 검색결과 페이지 스크롤 #######\n",
    "# 최초 스크롤 바의 높이값 읽기\n",
    "last_height = driver.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "#스크롤 횟수\n",
    "scroll_cnt = 0\n",
    "\n",
    "while True:\n",
    "    # 윈도우의 스크롤 바를 0에서부터 가장 밑(scrollHeight)까지 이동\n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
    "    scroll_cnt += 1\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "    # 스크롤 바 이동으로 새로운 검색 결과가 로딩 후 변경된 새로운 스크롤 바의 높이값 읽기\n",
    "    new_height = driver.execute_script('return document.body.scrollHeight')\n",
    "    print(f'last height : {last_height}, new_height : {new_height}, scroll count : {scroll_cnt}')\n",
    "    if last_height != new_height: # 계속해서 new_height 값이 변경되면\n",
    "        last_height = new_height\n",
    "    else: # 더 이상 new_height 값의 변동이 없다면\n",
    "        break\n",
    "\n",
    "######## 이미지 선택 및 해당 이미지의 src 추출 #######\n",
    "imgs = driver.find_elements(By.CSS_SELECTOR,'#rso > div > div > div.wH6SXe.u32vCb > div > div > div > div.czzyk.XOEbc > h3 > a')\n",
    "print(len(imgs))\n",
    "img_src_list = []\n",
    "img_cnt = 0\n",
    "\n",
    "for img in imgs:\n",
    "    try:\n",
    "        img_cnt += 1\n",
    "        img.click()\n",
    "        img_src = driver.find_element(By.XPATH, '//*[@id=\"Sva75c\"]/div[2]/div[2]/div[2]/div[2]/c-wiz/div/div/div/div/div[3]/div[1]/a/img[1]').get_attribute('src')\n",
    "        print(f'[{img_cnt}].{img_src}')\n",
    "        img_src_list.append(img_src)\n",
    "        if img_cnt == IMAGE_EXTRACT_NUM: break\n",
    "    except: # 에러가 발생했을 때 pass 해서 에러난 부분만 건너뛰고 실행\n",
    "        img_cnt -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feff5fb6-7d7f-4bf8-adb4-4b887c31aa4a",
   "metadata": {},
   "source": [
    "### 구글에서 이미지 검색 후 파일로 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fa31053-b5a7-43ae-93fb-876554355226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last height : 7126, new_height : 13915, scroll count : 1\n",
      "last height : 13915, new_height : 20363, scroll count : 2\n",
      "last height : 20363, new_height : 26966, scroll count : 3\n",
      "last height : 26966, new_height : 29657, scroll count : 4\n",
      "last height : 29657, new_height : 29657, scroll count : 5\n",
      "438\n",
      "[1].https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Python-logo-notext.svg/800px-Python-logo-notext.svg.png\n",
      "[2].https://velog.velcdn.com/images/deep-of-machine/post/3f778fa2-2b43-42b3-9233-091424be7d73/image.png\n",
      "[3].https://i.namu.wiki/i/mxMv5lNX8m8lUwu7yTjN6eNZh8JVuI6a_chEyMRc4V9oECkhVIl7OiPiGIOllv14uDVNuwRPVco8abCPe5xOiQ.svg\n",
      "[4].https://images.velog.io/images/pm1100tm/post/30e9dc94-96d0-41b3-9f2f-d814e839a796/python.jpg\n",
      "[5].https://i.namu.wiki/i/pHxeJONxIv51qQsN2ac5nX3shPEmiSlKtGVATZXUE22NHGyw9v7_Aqto6aSoCU9ODz3RKtTKCEP0E0OI7TlxMQ.webp\n",
      "[6].https://www.askedtech.com/api/kords/admin/product/image.jpg?type=org&id=20688\n",
      "[7].https://blog.kakaocdn.net/dn/cgkUIV/btqRqcabOMh/iSwGFjqyYk5pidLEb8K641/img.png\n",
      "[8].https://fineproxy.org/wp-content/uploads/2023/05/Python.jpg\n",
      "[9].https://www.snugarchive.com/static/06cc3be354c9abae90b6ebc9469d7ff1/84d4b/featured-image-python-logo.png\n",
      "[10].https://velog.velcdn.com/images/dmsgur7112/post/0d9e7592-ffa4-4843-a26d-7b74414c30e8/image.png\n",
      "[11].https://mblogthumb-phinf.pstatic.net/MjAyMjAyMTJfNSAg/MDAxNjQ0NTkzNzE5MzQ1.q5g3zqnCq2Rt1xUmpSFx2xWRQTl4VmngS8FGT7eGD0Ig.UKr_wLSCCg8PD-v8TfDddCKFIWhKoeqh5lZM09FVrsYg.PNG.sw4r/image.png?type=w800&jopt=2\n",
      "[12].https://store-images.s-microsoft.com/image/apps.46763.13825920798386907.6cc5afb4-6792-4e4b-ad02-2e1990cb8941.88dfa882-d514-400e-b8d7-52d6aafd6046?h=464\n",
      "[13].https://www.unite.ai/wp-content/uploads/2022/04/AI-Python-Libraries.png\n",
      "[14].https://www.udacity.com/blog/wp-content/uploads/2020/12/Python-Tutorial_Blog-scaled.jpeg\n",
      "[15].https://images.velog.io/images/taeil77/post/0860d033-75cf-4101-b236-1a261c8c2c8a/python.png\n",
      "[16].https://www.pcworld.com/wp-content/uploads/2023/04/python-100763894-orig-1.jpg?quality=50&strip=all\n",
      "[17].https://media.cnn.com/api/v1/images/stellar/prod/210707164757-louisiana-python-escape-file.jpg?q=x_2,y_808,h_898,w_1596,c_crop/h_833,w_1480\n",
      "[18].https://images.datacamp.com/image/upload/f_auto,q_auto:best/v1603718736/Why_Your_Company_Needs_Python_for_Business_Analytics_xzzles.png\n",
      "[19].https://upload.wikimedia.org/wikipedia/commons/1/10/Brooding_female_Python_molurus_bivittatus.jpg\n",
      "[20].https://miro.medium.com/v2/resize:fit:638/1*Wa4aTPXFq5Zv5bAKRVt3AA.png\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "SCROLL_PAUSE_TIME = 2# 페이지가 로드될 때까지 기다리는 시간\n",
    "IMAGE_EXTRACT_NUM = 20 # 이미지 추출 개수\n",
    "SEARCH_KEYWORD = 'python' \n",
    "\n",
    "######## 웹 드라이버 실행 및 구글 접속 #######\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "driver.get('https://www.google.com')\n",
    "\n",
    "######## 검색 #######\n",
    "ele = driver.find_element(by=By.CLASS_NAME, value='gLFyf')\n",
    "ele.send_keys(SEARCH_KEYWORD)\n",
    "ele.submit() #입력 폼을 서버로 전송할 때 \n",
    "\n",
    "######## 이미지 검색결과 페이지 이동 #######\n",
    "driver.find_element(By.LINK_TEXT, '이미지').click()\n",
    "\n",
    "# 페이지가 로드될 때까지 기다림\n",
    "time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "######## 이미지 검색결과 페이지 스크롤 #######\n",
    "# 최초 스크롤 바의 높이값 읽기\n",
    "last_height = driver.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "#스크롤 횟수\n",
    "scroll_cnt = 0\n",
    "\n",
    "while True:\n",
    "    # 윈도우의 스크롤 바를 0에서부터 가장 밑(scrollHeight)까지 이동\n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
    "    scroll_cnt += 1\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "    # 스크롤 바 이동으로 새로운 검색 결과가 로딩 후 변경된 새로운 스크롤 바의 높이값 읽기\n",
    "    new_height = driver.execute_script('return document.body.scrollHeight')\n",
    "    print(f'last height : {last_height}, new_height : {new_height}, scroll count : {scroll_cnt}')\n",
    "    if last_height != new_height: # 계속해서 new_height 값이 변경되면\n",
    "        last_height = new_height\n",
    "    else: # 더 이상 new_height 값의 변동이 없다면\n",
    "        break\n",
    "\n",
    "######## 이미지 선택 및 해당 이미지의 src 추출 #######\n",
    "imgs = driver.find_elements(By.CSS_SELECTOR,'#rso > div > div > div.wH6SXe.u32vCb > div > div > div > div.czzyk.XOEbc > h3 > a')\n",
    "print(len(imgs))\n",
    "img_src_list = []\n",
    "img_cnt = 0\n",
    "\n",
    "for img in imgs:\n",
    "    try:\n",
    "        img_cnt += 1\n",
    "        img.click()\n",
    "        time.sleep(2)\n",
    "        img_src = driver.find_element(By.CSS_SELECTOR, '#Sva75c > div.A8mJGd.NDuZHe.OGftbe-N7Eqid-H9tDt > div.LrPjRb > div.AQyBn > div.tvh9oe.BIB1wf > c-wiz > div > div > div > div > div.v6bUne > div.p7sI2.PUxBg > a > img.sFlh5c.pT0Scc.iPVvYb').get_attribute('src')\n",
    "        print(f'[{img_cnt}].{img_src}')\n",
    "        img_src_list.append(img_src)\n",
    "        if img_cnt == IMAGE_EXTRACT_NUM: break\n",
    "    except: # 에러가 발생했을 때 pass 해서 에러난 부분만 건너뛰고 실행\n",
    "        img_cnt -= 1\n",
    "\n",
    "######## 검색 이미지 파일로 저장 #######\n",
    "import os\n",
    "import requests\n",
    "\n",
    "ts = time.localtime()\n",
    "path = 'c:/Temp/'\n",
    "now = '{}.{}.{}.{}.{}.{}'.format(ts.tm_year, ts.tm_mon,ts.tm_mday, ts.tm_hour, ts.tm_min, ts.tm_sec)\n",
    "dir = SEARCH_KEYWORD + '/' + now+'/'\n",
    "os.chdir(path)\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir) # makedirs -> 폴더 2개 만들 때 사용 (SEARCH_KEYWORD 폴더, NOW 폴더)\n",
    "file_no = 1\n",
    "os.chdir(path+dir)\n",
    "for url in img_src_list:\n",
    "    extension = url.split('.') # 원본 이미지에서 가져온 확장자\n",
    "    ext = '' # 최종적으로 사용할 이미지의 확장자\n",
    "    if extension in ['jpg','JPG','jpeg','JPEG','png','PNG','gif','GIF']:\n",
    "        ext = '.'+ extension\n",
    "    else:\n",
    "        ext = '.jpg'\n",
    "\n",
    "    file_name = str(file_no) + '-' + SEARCH_KEYWORD + ext\n",
    "    file_no += 1\n",
    "    res = requests.get(url)\n",
    "    with open(file_name, 'wb') as f:\n",
    "        f.write(res.content)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e38c13-c47f-4f30-8ec3-b554c4126d2c",
   "metadata": {},
   "source": [
    "## select 태그 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2044681e-9901-4e92-accd-a7dbbb174179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import Select # select form 제어 클래스\n",
    "\n",
    "URL = 'https://www.selenium.dev/selenium/web/formPage.html'\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "driver.get(URL)\n",
    "\n",
    "ele = driver.find_element(By.NAME, 'selectomatic')\n",
    "select = Select(ele)\n",
    "\n",
    "# 인덱스 기준으로 선택하기\n",
    "# select.select_by_index(2)\n",
    "\n",
    "# 보여지는 선택값 텍스트로 선택하기\n",
    "# select.select_by_visible_text('Four')\n",
    "\n",
    "# option 요소의 값으로 선택하기\n",
    "# select.select_by_value('four')\n",
    "\n",
    "# select 태그에 onchange 속성이 있어서 Select 클래스를 사용할 수 없을 때\n",
    "driver.find_element(By.CSS_SELECTOR, 'option[value=\"four\"]').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c5515-3921-4c79-8364-9e8e8c24e768",
   "metadata": {},
   "source": [
    "[실습과제] yes24에서 파이썬 도서 검색하기\n",
    "- yes24 사이트에서 파이썬 도서 검색 -> 검색 결과를 120개 선택\n",
    "- 검색 결과에서 도서 평점이 9.6 이상인 도서 제목과 가격, 평점 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd5c734d-2b9f-46bd-9ba5-2cf51380e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1단계 : Selenium\n",
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "URL = 'https://www.yes24.com/Main/default.aspx'\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "driver.get(URL)\n",
    "\n",
    "ele = driver.find_element(By.ID, 'query')\n",
    "ele.send_keys('파이썬')\n",
    "ele.send_keys(Keys.ENTER)\n",
    "import time\n",
    "time.sleep(2)\n",
    "\n",
    "# 검색 결과를 120개 선택 \n",
    "driver.find_element(By.CSS_SELECTOR, '#stat_gb > option[value=\"120\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd742f35-e314-4ade-b351-5d1ebef72270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색 도서 권수 :  524\n",
      "001 | Do it! 점프 투 파이썬 | 19,800 원 | 9.8\n",
      "001 | Do it! 점프 투 파이썬 | 19,800 원 | 4.0\n",
      "001 | Do it! 점프 투 파이썬 | 19,800 원 | 9.3\n",
      "002 | 실무로 통하는 인과추론 with 파이썬 | 34,200 원 | 9.8\n",
      "002 | 실무로 통하는 인과추론 with 파이썬 | 34,200 원 | 9.4\n",
      "003 | 혼자 공부하는 머신러닝+딥러닝 | 23,400 원 | 9.6\n",
      "004 | 챗GPT API를 활용한 챗봇 만들기 | 28,800 원 | 10.0\n",
      "005 | 코딩 자율학습 나도코딩의 파이썬 입문 | 21,600 원 | 9.8\n",
      "006 | Hey, 파이썬! 생성형 AI 활용 앱 만들어 줘 | 35,100 원 | 9.8\n",
      "006 | Hey, 파이썬! 생성형 AI 활용 앱 만들어 줘 | 35,100 원 | 9.5\n",
      "006 | Hey, 파이썬! 생성형 AI 활용 앱 만들어 줘 | 35,100 원 | 9.0\n",
      "006 | Hey, 파이썬! 생성형 AI 활용 앱 만들어 줘 | 35,100 원 | 8.0\n",
      "007 | 챗GPT로 만드는 주식 & 암호화폐 자동매매 시스템 | 18,000 원 | 9.8\n",
      "008 | 혼자 공부하는 데이터 분석 with 파이썬 | 23,400 원 | 9.7\n",
      "009 | 마이크로 파이썬을 활용해 사물인터넷(IoT) 프로젝트 만들기 with ESP32 | 20,700 원 | 10.0\n",
      "010 | 새내기 파이썬 | 30,000 원 | 10.0\n",
      "010 | 새내기 파이썬 | 30,000 원 | 8.5\n",
      "011 | ChatGPT 소스를 얹는 파이썬 레시피 | 28,800 원 | 10.0\n",
      "012 | 으뜸 파이썬 | 32,000 원 | 9.6\n",
      "013 | 코딩 입문자를 위한 문제해결 기반 파이썬 4.0 | 27,000 원 | 10.0\n",
      "013 | 코딩 입문자를 위한 문제해결 기반 파이썬 4.0 | 27,000 원 | 9.1\n",
      "014 | 파이썬으로 쉽게 배우는 자료구조 | 29,000 원 | 10.0\n",
      "015 | 시간순삭 파이썬 | 22,500 원 | 10.0\n",
      "015 | 시간순삭 파이썬 | 22,500 원 | 6.0\n",
      "016 | 데이터 과학을 위한 기초수학 with 파이썬 | 23,400 원 | 10.0\n",
      "017 | Do it! 점프 투 파이썬 | 15,000 원 | 9.8\n",
      "017 | Do it! 점프 투 파이썬 | 15,000 원 | 9.2\n",
      "017 | Do it! 점프 투 파이썬 | 15,000 원 | 9.4\n",
      "018 | 데이터 과학을 위한 파이썬 프로그래밍 | 30,000 원 | 10.0\n",
      "018 | 데이터 과학을 위한 파이썬 프로그래밍 | 30,000 원 | 9.5\n",
      "019 | 자료구조와 알고리즘 with 파이썬 | 21,600 원 | 9.8\n",
      "019 | 자료구조와 알고리즘 with 파이썬 | 21,600 원 | 9.5\n",
      "019 | 자료구조와 알고리즘 with 파이썬 | 21,600 원 | 9.4\n",
      "019 | 자료구조와 알고리즘 with 파이썬 | 21,600 원 | 9.3\n",
      "019 | 자료구조와 알고리즘 with 파이썬 | 21,600 원 | 8.4\n",
      "019 | 자료구조와 알고리즘 with 파이썬 | 21,600 원 | 9.0\n",
      "020 | 코딩 테스트 합격자 되기 - 파이썬 편 | 36,000 원 | 9.9\n",
      "020 | 코딩 테스트 합격자 되기 - 파이썬 편 | 36,000 원 | 9.5\n",
      "021 | 파이썬을 이용한 퀀트 투자 포트폴리오 만들기 | 27,000 원 | 10.0\n",
      "022 | 데이터 과학 기반의 파이썬 빅데이터 분석 | 28,000 원 | 10.0\n",
      "022 | 데이터 과학 기반의 파이썬 빅데이터 분석 | 28,000 원 | 9.4\n",
      "023 | Do it! 쉽게 배우는 파이썬 데이터 분석 | 20,700 원 | 9.8\n",
      "023 | Do it! 쉽게 배우는 파이썬 데이터 분석 | 20,700 원 | 7.5\n",
      "023 | Do it! 쉽게 배우는 파이썬 데이터 분석 | 20,700 원 | 8.6\n",
      "023 | Do it! 쉽게 배우는 파이썬 데이터 분석 | 20,700 원 | 9.3\n",
      "024 | 파이썬 데이터 분석 & 시각화 + 웹 대시보드 제작하기 | 15,930 원 | 9.7\n",
      "025 | 난생처음 데이터 분석 with 파이썬 | 26,000 원 | 10.0\n",
      "026 | Do it! 게임 10개 만들며 배우는 파이썬 | 19,800 원 | 10.0\n",
      "026 | Do it! 게임 10개 만들며 배우는 파이썬 | 19,800 원 | 9.4\n",
      "026 | Do it! 게임 10개 만들며 배우는 파이썬 | 19,800 원 | 9.5\n",
      "027 | Do it! 첫 파이썬 | 12,600 원 | 9.8\n",
      "028 | 나는 파이썬으로 머신러닝한다 1 | 19,800 원 | 10.0\n",
      "029 | Do it! 첫 코딩 with 파이썬 | 16,200 원 | 9.9\n",
      "029 | Do it! 첫 코딩 with 파이썬 | 16,200 원 | 9.3\n",
      "029 | Do it! 첫 코딩 with 파이썬 | 16,200 원 | 8.0\n",
      "030 | 혼자 공부하는 머신러닝+딥러닝 | 20,800 원 | 9.6\n",
      "031 | 모두의 파이썬 | 10,800 원 | 9.6\n",
      "031 | 모두의 파이썬 | 10,800 원 | 8.0\n",
      "032 | 컴퓨팅 사고 with 파이썬 | 25,000 원 | 10.0\n",
      "033 | 파이썬 코딩 도장 | 27,000 원 | 9.8\n",
      "033 | 파이썬 코딩 도장 | 27,000 원 | 9.3\n",
      "033 | 파이썬 코딩 도장 | 27,000 원 | 9.1\n",
      "033 | 파이썬 코딩 도장 | 27,000 원 | 8.7\n",
      "034 | 컴퓨팅 사고와 파이썬 | 24,000 원 | 10.0\n",
      "035 | 파이썬 라이브러리를 활용한 머신러닝 | 29,700 원 | 9.9\n",
      "036 | 소프트웨어 코딩 대회를 위한 파이썬 문제 풀이 100 | 18,000 원 | 9.8\n",
      "036 | 소프트웨어 코딩 대회를 위한 파이썬 문제 풀이 100 | 18,000 원 | 8.4\n",
      "036 | 소프트웨어 코딩 대회를 위한 파이썬 문제 풀이 100 | 18,000 원 | 8.6\n",
      "036 | 소프트웨어 코딩 대회를 위한 파이썬 문제 풀이 100 | 18,000 원 | 8.3\n",
      "036 | 소프트웨어 코딩 대회를 위한 파이썬 문제 풀이 100 | 18,000 원 | 9.3\n",
      "037 | 혼자 공부하는 데이터 분석 with 파이썬 | 20,800 원 | 9.7\n",
      "038 | 나는 파이썬으로 머신러닝한다 2 | 20,700 원 | 10.0\n",
      "039 | 모두의 인공지능 with 파이썬 | 19,800 원 | 10.0\n",
      "040 | 실무로 통하는 인과추론 with 파이썬 | 30,400 원 | 9.8\n",
      "040 | 실무로 통하는 인과추론 with 파이썬 | 30,400 원 | 9.3\n",
      "040 | 실무로 통하는 인과추론 with 파이썬 | 30,400 원 | 7.0\n",
      "041 | 파이썬 기초 문법 | 0 원 | 10.0\n",
      "042 | 챗GPT API를 활용한 챗봇 만들기 | 25,600 원 | 10.0\n",
      "043 | 머신 러닝 교과서 with 파이썬, 사이킷런, 텐서플로 | 39,600 원 | 10.0\n",
      "043 | 머신 러닝 교과서 with 파이썬, 사이킷런, 텐서플로 | 39,600 원 | 9.3\n",
      "044 | 챗GPT로 만드는 주식 & 암호화폐 자동매매 시스템 | 14,000 원 | 9.8\n",
      "045 | 밑바닥부터 시작하는 딥러닝 2 | 26,100 원 | 9.8\n",
      "046 | 쉽게 따라 만드는 파이썬 주식 자동매매 시스템 | 25,200 원 | 9.6\n",
      "046 | 쉽게 따라 만드는 파이썬 주식 자동매매 시스템 | 25,200 원 | 9.5\n",
      "046 | 쉽게 따라 만드는 파이썬 주식 자동매매 시스템 | 25,200 원 | 9.5\n",
      "047 | 컴퓨터 비전과 딥러닝 | 39,000 원 | 9.6\n",
      "048 | 컴퓨터 사이언스 부트캠프 with 파이썬 | 21,600 원 | 10.0\n",
      "049 | 사장님 몰래하는 파이썬 업무 자동화 | 28,800 원 | 9.7\n",
      "050 | 창의적 문제 해결을 위한 파이썬 프로그래밍 | 23,000 원 | 10.0\n",
      "051 | 머신 러닝·딥 러닝에 필요한 기초 수학 with 파이썬 | 24,300 원 | 10.0\n",
      "052 | 파이썬 텍스트 마이닝 완벽 가이드 | 27,000 원 | 10.0\n",
      "052 | 파이썬 텍스트 마이닝 완벽 가이드 | 27,000 원 | 9.2\n",
      "052 | 파이썬 텍스트 마이닝 완벽 가이드 | 27,000 원 | 9.4\n",
      "052 | 파이썬 텍스트 마이닝 완벽 가이드 | 27,000 원 | 9.4\n",
      "053 | 파이썬으로 경험하는 빅데이터 분석과 머신러닝 | 27,000 원 | 10.0\n",
      "054 | 파이썬으로 배우는 소프트웨어와 인공지능 | 25,000 원 | 10.0\n"
     ]
    }
   ],
   "source": [
    "# 2단계 : 데이터 가져오기 (BeautifulSoup)\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml') # page_source -> 현재 웹드라이브의 html을 가져옴\n",
    "book_list = soup.find('ul', attrs = {'id':'yesSchList'})\n",
    "books = book_list.find_all('li') # 속성 조합 안 넣는 이유 : 저 ul 아래 있는 li 라서\n",
    "\n",
    "print('검색 도서 권수 : ', len(books)) \n",
    "count = 0\n",
    "\n",
    "for book in books:\n",
    "    # price의 em 태그의 클래스 yes_b 가 rating이랑 겹쳐서 상위 태그인 span을 가져옴 (price가 먼저 나오기 때문에 find를 쓰려면 여기다가 써야 함) \n",
    "    rating = book.find('span', attrs = {'class':'rating_grade'})\n",
    "    if not rating: continue # 평점이 없는 도서일 경우 이하 내용 실행 skip\n",
    "    # span 태그 밑에 2개의 하위 태그가 있어서 특정한 태그인 'em'을 find 해줌 \n",
    "    rating = float(rating.find('em').get_text())\n",
    "    \n",
    "    if rating >= 9.6:\n",
    "        count += 1 \n",
    "        title = book.find('a', attrs = {'class':'gd_name'}).get_text()\n",
    "        price = book.find('em', attrs = {'class':'yes_b'}).get_text() # '(strong', attrs = {'class':'txt_num'})\n",
    "    print(f'{count :03d} | {title} | {price} 원 | {rating}') #03d 세자리의 정수 자리 유지 (앞에 빈 부분은 0으로 메움)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96e0173-2da7-4f02-80b7-545812edc01f",
   "metadata": {},
   "source": [
    "[실습] 메가박스 영화 감상평 및 평점 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "50df18e9-479a-41ce-81e2-4791422df8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "PAUSE_TIME = 3\n",
    "\n",
    "URL = 'https://www.megabox.co.kr/movie'\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "driver.get(URL)\n",
    "\n",
    "movie = driver.find_element(By.CSS_SELECTOR, '#movieList > li:nth-child(4) > div.movie-list-info > div.movie-score > a').send_keys(Keys.ENTER) #.click() 안 먹히면 .send_keys(Keys.ENTER)\n",
    "time.sleep(PAUSE_TIME)\n",
    "driver.find_element(By.LINK_TEXT, '실관람평').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ffcae18-c90e-48fb-a6db-381bae7fda53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 페이지 수 :  2555\n",
      "1페이지 평점 및 리뷰 정보 수집\n",
      "2페이지 평점 및 리뷰 정보 수집\n",
      "3페이지 평점 및 리뷰 정보 수집\n",
      "4페이지 평점 및 리뷰 정보 수집\n",
      "5페이지 평점 및 리뷰 정보 수집\n",
      "6페이지 평점 및 리뷰 정보 수집\n",
      "7페이지 평점 및 리뷰 정보 수집\n",
      "8페이지 평점 및 리뷰 정보 수집\n",
      "9페이지 평점 및 리뷰 정보 수집\n",
      "10페이지 평점 및 리뷰 정보 수집\n",
      "11페이지 평점 및 리뷰 정보 수집\n",
      "12페이지 평점 및 리뷰 정보 수집\n",
      "13페이지 평점 및 리뷰 정보 수집\n",
      "14페이지 평점 및 리뷰 정보 수집\n",
      "15페이지 평점 및 리뷰 정보 수집\n",
      "16페이지 평점 및 리뷰 정보 수집\n",
      "17페이지 평점 및 리뷰 정보 수집\n",
      "18페이지 평점 및 리뷰 정보 수집\n",
      "19페이지 평점 및 리뷰 정보 수집\n",
      "20페이지 평점 및 리뷰 정보 수집\n"
     ]
    }
   ],
   "source": [
    "# 마지막 페이지 이동 \n",
    "driver.find_element(By.CSS_SELECTOR, '#contentData > div > div.movie-idv-story > nav > a.control.last').click()\n",
    "time.sleep(PAUSE_TIME)\n",
    "total_page_num = int(driver.find_element(By.CSS_SELECTOR, '#contentData > div > div.movie-idv-story > nav > strong').text)\n",
    "print('전체 페이지 수 : ', total_page_num)\n",
    "\n",
    "# 첫 페이지 이동\n",
    "driver.find_element(By.CSS_SELECTOR, '#contentData > div > div.movie-idv-story > nav > a.control.first').click()\n",
    "time.sleep(PAUSE_TIME)\n",
    "\n",
    "ratings = [] \n",
    "comments = []\n",
    "\n",
    "THE_LAST_PAGE = 20\n",
    "next_a_tag = 2 # 다음 클릭이 이루어질 a 태그의 순서 (첫 페이지는 a 태그가 아닌 strong 태그이기 때문에 첫 클릭이 이뤄질 태그 순서는 2부터 시작)\n",
    "\n",
    "for i in range(1, total_page_num+1):\n",
    "    if i > THE_LAST_PAGE: break\n",
    "    print(f'{i}페이지 평점 및 리뷰 정보 수집')\n",
    "    bs = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    result = bs.find_all('li', attrs = {'class':'type01 oneContentTag'})\n",
    "    for c in result:\n",
    "        rating = int(c.find('div', attrs = {'class':'story-point'}).text)\n",
    "        comment = c.find('div', attrs = {'class':'story-txt'}).text.strip()\n",
    "        ratings.append(rating)\n",
    "        comments.append(comment)\n",
    "\n",
    "    if not i % 10: # 10번째 페이지 평점 정보 수집이 끝나면...\n",
    "        driver.find_element(By.CSS_SELECTOR, '#contentData > div > div.movie-idv-story > nav > a.control.next').click() # 다음 10페이지 보기 클릭\n",
    "        time.sleep(PAUSE_TIME)\n",
    "        next_a_tag = 4 # 다음 10페이지에서 그 다음 클릭이 이뤄질 a 태그의 순서는 4가 됨(다음 클릭할 페이지 링크 앞에 <, <<, 첫페이지 가 있어서..)\n",
    "        continue\n",
    "        \n",
    "    # 다음 페이지 번호 클릭\n",
    "    driver.find_element(By.CSS_SELECTOR, f'#contentData > div > div.movie-idv-story > nav > a:nth-child({next_a_tag})').click()\n",
    "    next_a_tag += 1\n",
    "    time.sleep(PAUSE_TIME)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd3abccf-2a8d-4460-a434-0bd7a3bd71e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "관람객 평점 : 9.2점\n",
      "평점 저장 완료\n",
      "감상평 저장 완료\n"
     ]
    }
   ],
   "source": [
    "print(f'관람객 평점 : {sum(ratings)/len(ratings):.1f}점')\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('ratings.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for rating in ratings:\n",
    "        writer.writerow([rating]) # 한 행에 써야 할 데이터는 꼭 리스트의 형태로\n",
    "print('평점 저장 완료')\n",
    "\n",
    "with open('comment.txt', 'w', encoding = 'utf-8') as f:\n",
    "    for comment in comments:\n",
    "        f.write(comment+'\\n')\n",
    "print('감상평 저장 완료')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
